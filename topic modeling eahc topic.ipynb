{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede74c40-70c3-4cfa-83a7-0888f60a7713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: * PHILOSOPHY OF FURNITURE (21 chunks)\n",
      "  ✅ Topic 0: aristocracy, taste, display, sole, readily (5/21 chunks, 23.8%)\n",
      "Processing: A DESCENT INTO THE MAELSTRÖM (61 chunks)\n",
      "  ✅ Topic 0: ström, slack, moskoe ström, moskoe, whirl (9/61 chunks, 14.8%)\n",
      "Processing: A PREDICAMENT (33 chunks)\n",
      "  ✅ Topic 0: pompey, diana, said, hole, city (26/33 chunks, 78.8%)\n",
      "Processing: A TALE OF JERUSALEM (10 chunks)\n",
      "  ✅ Topic 0: let, wall, day, city, phittim (3/10 chunks, 30.0%)\n",
      "Processing: A TALE OF THE RAGGED MOUNTAINS (36 chunks)\n",
      "  ✅ Topic 0: came, felt, tree, astonishment, soul (9/36 chunks, 25.0%)\n",
      "Processing: BERENICE (27 chunks)\n",
      "  ✅ Topic 0: est, disorder, nature, horizon rainbow, trance (5/27 chunks, 18.5%)\n",
      "Processing: DIDDLING (37 chunks)\n",
      "  ✅ Topic 0: diddler, diddle, brandy water, brandy, water (8/37 chunks, 21.6%)\n",
      "Processing: ELEONORA (19 chunks)\n",
      "  ✅ Topic 0: valley, love, loveliness, maiden, eleonora (4/19 chunks, 21.1%)\n",
      "Processing: FOUR BEASTS IN ONE—THE HOMO-CAMELEOPARD (25 chunks)\n",
      "  ✅ Topic 1: animal, epidaphne, terrible, dangerous, nature (4/25 chunks, 16.0%)\n",
      "Processing: HOP-FROG (29 chunks)\n",
      "  ✅ Topic 0: ourang, outangs, ourang outangs, masqueraders, leave (5/29 chunks, 17.2%)\n",
      "Processing: HOW TO WRITE A BLACKWOOD ARTICLE (35 chunks)\n",
      "  ✅ Topic 0: write, chicken, bone, chicken bone, piquant (10/35 chunks, 28.6%)\n",
      "Processing: KING PEST (37 chunks)\n",
      "  ✅ Topic 0: plague, terror, death, hand, streets (6/37 chunks, 16.2%)\n",
      "Processing: LANDOR’S COTTAGE (41 chunks)\n",
      "  ✅ Topic 0: sun, valley, gentle, scarcely, scene (5/41 chunks, 12.2%)\n",
      "Processing: LIGEIA (51 chunks)\n",
      "  ✅ Topic 0: ligeia, eyes, beauty, far, lady (27/51 chunks, 52.9%)\n",
      "Processing: LIONIZING (14 chunks)\n",
      "  ✅ Topic 0: said, nose, father, nosology, man (5/14 chunks, 35.7%)\n",
      "Processing: LOSS OF BREATH (40 chunks)\n",
      "  ✅ Topic 0: breath, et, et cetera, cetera, conversation (7/40 chunks, 17.5%)\n",
      "Processing: MAELZEL’S CHESS-PLAYER (80 chunks)\n",
      "  ✅ Topic 0: automaton, machine, chess, arm, maelzel (47/80 chunks, 58.8%)\n",
      "Processing: MELLONTA TAUTA (50 chunks)\n",
      "  ✅ Topic 0: pundit, know, april, balloon, sea (20/50 chunks, 40.0%)\n",
      "Processing: MESMERIC REVELATION (32 chunks)\n",
      "  ✅ Topic 0: sleep, mesmeric, long, condition, death (11/32 chunks, 34.4%)\n",
      "Processing: METZENGERSTEIN (24 chunks)\n",
      "  ✅ Topic 0: tapestry, expression, direction, glance, position (4/24 chunks, 16.7%)\n",
      "Processing: MORELLA (17 chunks)\n",
      "  ✅ Topic 0: morella, days, thou, spirit, earth (10/17 chunks, 58.8%)\n",
      "Processing: MS FOUND IN A BOTTLE (36 chunks)\n",
      "  ✅ Topic 0: ship, like, sea, horror, ocean (10/36 chunks, 27.8%)\n",
      "Processing: MYSTIFICATION (25 chunks)\n",
      "  ✅ Topic 0: length, especially, etiquette, hermann, time (5/25 chunks, 20.0%)\n",
      "Processing: NEVER BET THE DEVIL YOUR HEAD (34 chunks)\n",
      "  ✅ Topic 0: dammit, mr, did, head, said (30/34 chunks, 88.2%)\n",
      "Processing: OLD ENGLISH POETRY (13 chunks)\n",
      "  ✅ Topic 0: general, delight, poems, school, old (7/13 chunks, 53.8%)\n",
      "Processing: SHADOW—A PARABLE (8 chunks)\n",
      "  ✅ Topic 0: shadow, god, draperies, shall, seen (4/8 chunks, 50.0%)\n",
      "Processing: SILENCE—A FABLE (11 chunks)\n",
      "  ✅ Topic 0: river, forever, rain, unto, everlasting (3/11 chunks, 27.3%)\n",
      "Processing: SOME WORDS WITH A MUMMY (53 chunks)\n",
      "  ✅ Topic 0: doctor, mr, right, ponnonner, doctor ponnonner (14/53 chunks, 26.4%)\n",
      "Processing: THE ANGEL OF THE ODD (33 chunks)\n",
      "  ✅ Topic 0: te, angel, pe, und, said (11/33 chunks, 33.3%)\n",
      "Processing: THE ASSIGNATION (39 chunks)\n",
      "  ✅ Topic 0: ha, ha ha, passion, spirit, stranger (10/39 chunks, 25.6%)\n",
      "Processing: THE BALLOON-HOAX (47 chunks)\n",
      "  ✅ Topic 0: rope, balloon, gas, ballast, pounds (9/47 chunks, 19.1%)\n",
      "Processing: THE BLACK CAT (35 chunks)\n",
      "  ✅ Topic 0: wall, walls, corpse, stood, house (6/35 chunks, 17.1%)\n",
      "Processing: THE BUSINESS MAN (33 chunks)\n",
      "  ✅ Topic 0: course, mud, palace, little, corporation (6/33 chunks, 18.2%)\n",
      "Processing: THE CASK OF AMONTILLADO (22 chunks)\n",
      "  ✅ Topic 0: ugh, ugh ugh, said, replied, yes (5/22 chunks, 22.7%)\n",
      "Processing: THE COLLOQUY OF MONOS AND UNA (29 chunks)\n",
      "  ✅ Topic 0: man, sentiment, earth, intellect, sweet (23/29 chunks, 79.3%)\n",
      "Processing: THE CONVERSATION OF EIROS AND CHARMION (19 chunks)\n",
      "  ✅ Topic 0: comet, men, contact, wild, astronomers (15/19 chunks, 78.9%)\n",
      "Processing: THE DEVIL IN THE BELFRY (27 chunks)\n",
      "  ✅ Topic 0: vondervotteimittiss, borough, steeple, village, inhabitants (5/27 chunks, 18.5%)\n",
      "Processing: THE DOMAIN OF ARNHEIM (49 chunks)\n",
      "  ✅ Topic 0: nature, beauty, ellison, art, landscape (29/49 chunks, 59.2%)\n",
      "Processing: THE DUC DE L’OMELETTE (11 chunks)\n",
      "  ✅ Topic 0: duc, grace, majesty, omelette, sir (8/11 chunks, 72.7%)\n",
      "Processing: THE FACTS IN THE CASE OF M VALDEMAR (31 chunks)\n",
      "  ✅ Topic 1: sound, voice, valdemar, appearance, general (5/31 chunks, 16.1%)\n",
      "Processing: THE FALL OF THE HOUSE OF USHER (56 chunks)\n",
      "  ✅ Topic 0: certain, terror, condition, chamber, said (14/56 chunks, 25.0%)\n",
      "Processing: THE GOLD-BUG (115 chunks)\n",
      "  ✅ Topic 0: jupiter, legrand, massa, tree, bug (97/115 chunks, 84.3%)\n",
      "Processing: THE IMP OF THE PERVERSE (22 chunks)\n",
      "  ✅ Topic 0: man, combativeness, organ, phrenology, wrong (7/22 chunks, 31.8%)\n",
      "Processing: THE ISLAND OF THE FAY (15 chunks)\n",
      "  ✅ Topic 0: regard, behold, solitude, life, belle (3/15 chunks, 20.0%)\n",
      "Processing: THE LANDSCAPE GARDEN (27 chunks)\n",
      "  ✅ Topic 0: ellison, bliss, extraordinary, laws, life (4/27 chunks, 14.8%)\n",
      "Processing: THE LITERARY LIFE OF  THINGUM BOB, ESQ (60 chunks)\n",
      "  ✅ Topic 0: bob, oil, oil bob, fly, mr (21/60 chunks, 35.0%)\n",
      "Processing: THE MAN OF THE CROWD (30 chunks)\n",
      "  ✅ Topic 0: man, old, street, seen, came (23/30 chunks, 76.7%)\n",
      "Processing: THE MAN THAT WAS USED UP (32 chunks)\n",
      "  ✅ Topic 0: general, bugaboo, just, brevet, brevet brigadier (5/32 chunks, 15.6%)\n",
      "Processing: THE MASQUE OF THE RED DEATH (21 chunks)\n",
      "  ✅ Topic 0: prince, death, prospero, prince prospero, red (10/21 chunks, 47.6%)\n",
      "Processing: THE MURDERS IN THE RUE MORGUE (126 chunks)\n",
      "  ✅ Topic 0: voice, said, head, dupin, room (99/126 chunks, 78.6%)\n",
      "Processing: THE MYSTERY OF MARIE ROGET (172 chunks)\n",
      "  ✅ Topic 0: marie, corpse, body, rog, madame (92/172 chunks, 53.5%)\n",
      "Processing: THE OBLONG BOX (39 chunks)\n",
      "  ✅ Topic 0: conclusion, time, box, supper, nicolino (6/39 chunks, 15.4%)\n",
      "Processing: THE OVAL PORTRAIT (11 chunks)\n",
      "  ✅ Topic 0: paintings, deep, candelabrum, long, chateau (4/11 chunks, 36.4%)\n",
      "Processing: THE PIT AND THE PENDULUM (56 chunks)\n",
      "  ✅ Topic 0: circuit, prison, wall, dungeon, walls (9/56 chunks, 16.1%)\n",
      "Processing: THE POETIC PRINCIPLE (56 chunks)\n",
      "  ✅ Topic 0: truth, harmony, effect, mere, odors (5/56 chunks, 8.9%)\n",
      "Processing: THE POWER OF WORDS (11 chunks)\n",
      "  ✅ Topic 0: oinos, agathos, knowledge, know, cognizant (4/11 chunks, 36.4%)\n",
      "Processing: THE PREMATURE BURIAL (49 chunks)\n",
      "  ✅ Topic 0: coffin, grave, death, fearful, body (16/49 chunks, 32.7%)\n",
      "Processing: THE PURLOINED LETTER (58 chunks)\n",
      "  ✅ Topic 0: said, puff, dupin, ha, prefect (10/58 chunks, 17.2%)\n",
      "Processing: THE SPECTACLES (82 chunks)\n",
      "  ✅ Topic 0: eugenie, ami, mon, mon ami, sacrifice (9/82 chunks, 11.0%)\n",
      "Processing: THE SPHINX (16 chunks)\n",
      "  ✅ Topic 0: time, city, banks, hand, elapsed (3/16 chunks, 18.8%)\n",
      "Processing: THE SYSTEM OF DOCTOR TARR AND PROFESSOR FETHER (60 chunks)\n",
      "  ✅ Topic 0: monsieur, maillard, monsieur maillard, said, ma (17/60 chunks, 28.3%)\n",
      "Processing: THE TELL-TALE HEART (20 chunks)\n",
      "  ✅ Topic 0: increased, sound, steadily, noise, talked (4/20 chunks, 20.0%)\n",
      "Processing: THE THOUSAND-AND-SECOND TALE OF SCHEHERAZADE (40 chunks)\n",
      "  ✅ Topic 0: said, animals, king, said king, like (19/40 chunks, 47.5%)\n",
      "Processing: THE UNPARALLELED ADVENTURES OF ONE HANS PFAAL (160 chunks)\n",
      "  ✅ Topic 0: car, head, time, balloon, pigeons (21/160 chunks, 13.1%)\n",
      "Processing: THOU ART THE MAN (44 chunks)\n",
      "  ✅ Topic 0: box, eyes, wine, mr, corpse (5/44 chunks, 11.4%)\n",
      "Processing: THREE SUNDAYS IN A WEEK (22 chunks)\n",
      "  ✅ Topic 0: sunday, smitherton, pratt, uncle, year (9/22 chunks, 40.9%)\n",
      "Processing: VON KEMPELEN AND HIS DISCOVERY (22 chunks)\n",
      "  ✅ Topic 0: altogether, york, misanthrope, new, new york (4/22 chunks, 18.2%)\n",
      "Processing: WHY THE LITTLE FRENCHMAN WEARS HIS HAND IN A SLING (65 chunks)\n",
      "  ✅ Topic 0: bon, metaphysician, say, majesty, said (21/65 chunks, 32.3%)\n",
      "Processing: WILLIAM WILSON (71 chunks)\n",
      "  ✅ Topic 0: wilson, day, oxford, evil, set (21/71 chunks, 29.6%)\n",
      "Processing: X-ING A PARAGRAPH (20 chunks)\n",
      "  ✅ Topic 0: yxu, oh, gx, know, don (8/20 chunks, 40.0%)\n",
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "Total stories: 70\n",
      "Stories with topics: 70\n",
      "\n",
      "Topic distribution:\n",
      "topic_id\n",
      "0    68\n",
      "1     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "SAMPLE RESULTS\n",
      "============================================================\n",
      "                                     title topic_id  \\\n",
      "0             A DESCENT INTO THE MAELSTRÖM        0   \n",
      "1                            A PREDICAMENT        0   \n",
      "2                      A TALE OF JERUSALEM        0   \n",
      "3           A TALE OF THE RAGGED MOUNTAINS        0   \n",
      "4                                 BERENICE        0   \n",
      "5                                 DIDDLING        0   \n",
      "6                                 ELEONORA        0   \n",
      "7  FOUR BEASTS IN ONE—THE HOMO-CAMELEOPARD        1   \n",
      "8                                 HOP-FROG        0   \n",
      "9         HOW TO WRITE A BLACKWOOD ARTICLE        0   \n",
      "\n",
      "                                      topic_keywords num_chunks  \\\n",
      "0          ström, slack, moskoe ström, moskoe, whirl         61   \n",
      "1                    pompey, diana, said, hole, city         33   \n",
      "2                      let, wall, day, city, phittim         10   \n",
      "3               came, felt, tree, astonishment, soul         36   \n",
      "4     est, disorder, nature, horizon rainbow, trance         27   \n",
      "5       diddler, diddle, brandy water, brandy, water         37   \n",
      "6         valley, love, loveliness, maiden, eleonora         19   \n",
      "7     animal, epidaphne, terrible, dangerous, nature         25   \n",
      "8  ourang, outangs, ourang outangs, masqueraders,...         29   \n",
      "9        write, chicken, bone, chicken bone, piquant         35   \n",
      "\n",
      "  topic_confidence  \n",
      "0         0.147541  \n",
      "1         0.787879  \n",
      "2              0.3  \n",
      "3             0.25  \n",
      "4         0.185185  \n",
      "5         0.216216  \n",
      "6         0.210526  \n",
      "7             0.16  \n",
      "8         0.172414  \n",
      "9         0.285714  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load your data\n",
    "# --------------------------------------------------\n",
    "# Load the CHUNKED data (just for processing)\n",
    "chunks_df = pd.read_csv(\"samentic_chunk.csv\")\n",
    "chunks_df = chunks_df.sort_values([\"title\", \"chunk_id\"]).reset_index(drop=True)\n",
    "\n",
    "# Load the ORIGINAL stories data (where we'll save topics)\n",
    "stories_df = pd.read_csv(r\"C:\\Users\\diwas\\Downloads\\preprocessed_data.csv~1\\preprocessed_data.csv\")\n",
    "\n",
    "# Add topic columns to stories ONLY\n",
    "stories_df[\"topic_id\"] = None\n",
    "stories_df[\"topic_keywords\"] = None\n",
    "stories_df[\"num_chunks\"] = None\n",
    "stories_df[\"topic_confidence\"] = None\n",
    "\n",
    "# Shared embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Configure BERTopic components\n",
    "# --------------------------------------------------\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=5,\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=2,\n",
    "    min_samples=1,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom',\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    min_df=1,\n",
    "    ngram_range=(1, 2)\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Process ONE story at a time\n",
    "# --------------------------------------------------\n",
    "for title in chunks_df[\"title\"].unique():\n",
    "    story_chunks = chunks_df[chunks_df[\"title\"] == title]\n",
    "    chunks = story_chunks[\"chunk_text\"].tolist()\n",
    "    \n",
    "    print(f\"Processing: {title} ({len(chunks)} chunks)\")\n",
    "    \n",
    "    # Skip if too few chunks\n",
    "    if len(chunks) < 3:\n",
    "        story_idx = stories_df[stories_df[\"title\"] == title].index[0]\n",
    "        stories_df.loc[story_idx, \"topic_id\"] = -1\n",
    "        stories_df.loc[story_idx, \"topic_keywords\"] = \"insufficient_data\"\n",
    "        stories_df.loc[story_idx, \"num_chunks\"] = len(chunks)\n",
    "        print(f\"  ⚠️ Skipped (too few chunks)\")\n",
    "        continue\n",
    "    \n",
    "    # Embed chunks\n",
    "    embeddings = embedding_model.encode(chunks, show_progress_bar=False)\n",
    "    \n",
    "    # Topic modeling\n",
    "    try:\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            language=\"english\",\n",
    "            calculate_probabilities=False,\n",
    "            verbose=False,\n",
    "            nr_topics=\"auto\"\n",
    "        )\n",
    "        \n",
    "        chunk_topics, _ = topic_model.fit_transform(chunks, embeddings)\n",
    "        \n",
    "        # Find dominant topic\n",
    "        topic_counts = Counter(t for t in chunk_topics if t != -1)\n",
    "        \n",
    "        if topic_counts:\n",
    "            dominant_topic = topic_counts.most_common(1)[0][0]\n",
    "            topic_count = topic_counts[dominant_topic]\n",
    "            confidence = topic_count / len(chunks)\n",
    "            \n",
    "            # Get keywords for dominant topic\n",
    "            top_words = topic_model.get_topic(dominant_topic)\n",
    "            if top_words:\n",
    "                keywords = \", \".join([word for word, _ in top_words[:5]])\n",
    "            else:\n",
    "                keywords = \"no_keywords\"\n",
    "            \n",
    "            print(f\"  ✅ Topic {dominant_topic}: {keywords} ({topic_count}/{len(chunks)} chunks, {confidence:.1%})\")\n",
    "        else:\n",
    "            dominant_topic = -1\n",
    "            keywords = \"no_topic_found\"\n",
    "            confidence = 0.0\n",
    "            print(f\"  ⚠️ No topics found (all noise)\")\n",
    "        \n",
    "        # Store in STORIES dataframe (not chunks!)\n",
    "        story_idx = stories_df[stories_df[\"title\"] == title].index[0]\n",
    "        stories_df.loc[story_idx, \"topic_id\"] = dominant_topic\n",
    "        stories_df.loc[story_idx, \"topic_keywords\"] = keywords\n",
    "        stories_df.loc[story_idx, \"num_chunks\"] = len(chunks)\n",
    "        stories_df.loc[story_idx, \"topic_confidence\"] = confidence\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error: {str(e)}\")\n",
    "        story_idx = stories_df[stories_df[\"title\"] == title].index[0]\n",
    "        stories_df.loc[story_idx, \"topic_id\"] = -1\n",
    "        stories_df.loc[story_idx, \"topic_keywords\"] = \"error\"\n",
    "        stories_df.loc[story_idx, \"num_chunks\"] = len(chunks)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Save ONLY the stories with topics\n",
    "# --------------------------------------------------\n",
    "stories_df.to_csv(r\"C:\\Users\\diwas\\Downloads\\preprocessed_data.csv~1\\preprocessed_data.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total stories: {len(stories_df)}\")\n",
    "print(f\"Stories with topics: {(stories_df['topic_id'] != -1).sum()}\")\n",
    "print(f\"\\nTopic distribution:\")\n",
    "print(stories_df['topic_id'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(stories_df[['title', 'topic_id', 'topic_keywords', 'num_chunks', 'topic_confidence']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8088e8f-2a63-4fd3-8308-f53b19e93491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>token_count</th>\n",
       "      <th>topic_model</th>\n",
       "      <th>chunk_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>* PHILOSOPHY OF FURNITURE</td>\n",
       "      <td>0</td>\n",
       "      <td>In the internal decoration, if not in the exte...</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>* PHILOSOPHY OF FURNITURE</td>\n",
       "      <td>1</td>\n",
       "      <td>In Spain they are all curtains—a nation of han...</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>* PHILOSOPHY OF FURNITURE</td>\n",
       "      <td>2</td>\n",
       "      <td>To speak less abstractly. In England, for exam...</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>* PHILOSOPHY OF FURNITURE</td>\n",
       "      <td>3</td>\n",
       "      <td>The people will imitate the nobles, and the re...</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>* PHILOSOPHY OF FURNITURE</td>\n",
       "      <td>4</td>\n",
       "      <td>There could be nothing more directly offensive...</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2847</th>\n",
       "      <td>X-ING A PARAGRAPH</td>\n",
       "      <td>15</td>\n",
       "      <td>The true reason, perhaps, is that x is rather ...</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2848</th>\n",
       "      <td>X-ING A PARAGRAPH</td>\n",
       "      <td>16</td>\n",
       "      <td>Next morning the population of Nopolis were ta...</td>\n",
       "      <td>276</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849</th>\n",
       "      <td>X-ING A PARAGRAPH</td>\n",
       "      <td>17</td>\n",
       "      <td>The first definite idea entertained by the pop...</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>X-ING A PARAGRAPH</td>\n",
       "      <td>18</td>\n",
       "      <td>One gentleman thought the whole an X-ellent jo...</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2851</th>\n",
       "      <td>X-ING A PARAGRAPH</td>\n",
       "      <td>19</td>\n",
       "      <td>Even the town mathematician confessed that he ...</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2852 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          title  chunk_id  \\\n",
       "0     * PHILOSOPHY OF FURNITURE         0   \n",
       "1     * PHILOSOPHY OF FURNITURE         1   \n",
       "2     * PHILOSOPHY OF FURNITURE         2   \n",
       "3     * PHILOSOPHY OF FURNITURE         3   \n",
       "4     * PHILOSOPHY OF FURNITURE         4   \n",
       "...                         ...       ...   \n",
       "2847          X-ING A PARAGRAPH        15   \n",
       "2848          X-ING A PARAGRAPH        16   \n",
       "2849          X-ING A PARAGRAPH        17   \n",
       "2850          X-ING A PARAGRAPH        18   \n",
       "2851          X-ING A PARAGRAPH        19   \n",
       "\n",
       "                                             chunk_text  token_count  \\\n",
       "0     In the internal decoration, if not in the exte...          128   \n",
       "1     In Spain they are all curtains—a nation of han...          148   \n",
       "2     To speak less abstractly. In England, for exam...          128   \n",
       "3     The people will imitate the nobles, and the re...          156   \n",
       "4     There could be nothing more directly offensive...          184   \n",
       "...                                                 ...          ...   \n",
       "2847  The true reason, perhaps, is that x is rather ...          145   \n",
       "2848  Next morning the population of Nopolis were ta...          276   \n",
       "2849  The first definite idea entertained by the pop...          122   \n",
       "2850  One gentleman thought the whole an X-ellent jo...          133   \n",
       "2851  Even the town mathematician confessed that he ...          182   \n",
       "\n",
       "     topic_model chunk_topic  \n",
       "0              0           0  \n",
       "1              0           0  \n",
       "2              0           0  \n",
       "3              0           0  \n",
       "4              0           2  \n",
       "...          ...         ...  \n",
       "2847           0           3  \n",
       "2848           0           0  \n",
       "2849           0           2  \n",
       "2850           0           4  \n",
       "2851           0           4  \n",
       "\n",
       "[2852 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b909da11-2e72-4da1-bc03-3a8f7a59fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_df.to_csv(\"preprocessed_data_with_topics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c7139e-375d-436a-92ac-2cb4b436cb68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
